{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP47aeIg9YGYP/vMq09uO74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "818d700256a54a0c9d9c32416499970e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13450e5bf95c444896ef3c6b83cd0227",
              "IPY_MODEL_c7322645efdd4538b6770e64943ced50",
              "IPY_MODEL_1fbd674945dd4fa9a224424abf37b4a7"
            ],
            "layout": "IPY_MODEL_892eee6cd07f4e819e8c425877e78210"
          }
        },
        "13450e5bf95c444896ef3c6b83cd0227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839ad0282a1c489e8d132a13d8b8da43",
            "placeholder": "​",
            "style": "IPY_MODEL_f224e2e049f749d3a3930b5d68f83f2c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c7322645efdd4538b6770e64943ced50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3390a2b01af749f4b41e6a0b2a6814be",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8556d4475154664b88e200188781247",
            "value": 26
          }
        },
        "1fbd674945dd4fa9a224424abf37b4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8518126c084644d99e920708828d58a0",
            "placeholder": "​",
            "style": "IPY_MODEL_9372b26df9fe42599c9f6e7d46796c1c",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.93kB/s]"
          }
        },
        "892eee6cd07f4e819e8c425877e78210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839ad0282a1c489e8d132a13d8b8da43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f224e2e049f749d3a3930b5d68f83f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3390a2b01af749f4b41e6a0b2a6814be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8556d4475154664b88e200188781247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8518126c084644d99e920708828d58a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9372b26df9fe42599c9f6e7d46796c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e35c909dd040e89346827385311e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f10b123357748df8334b0d0aae3f176",
              "IPY_MODEL_2ae5c339ccae43d084c0760682fda056",
              "IPY_MODEL_39c3c2dd9b804ebba245bbc6ec3178b7"
            ],
            "layout": "IPY_MODEL_dc1502c68b5f4c6fb36b8806c4913c21"
          }
        },
        "1f10b123357748df8334b0d0aae3f176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc7cebc7ad264417badd837693fd34ee",
            "placeholder": "​",
            "style": "IPY_MODEL_a93992552b0f4f1db124fe86a8c00cf9",
            "value": "config.json: 100%"
          }
        },
        "2ae5c339ccae43d084c0760682fda056": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e505a627f3c454b8d03665f46b46599",
            "max": 1515,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f43eb5f616146c8bd3992ace0fd3b47",
            "value": 1515
          }
        },
        "39c3c2dd9b804ebba245bbc6ec3178b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf2c81a32fd348baa9985e477a4c8467",
            "placeholder": "​",
            "style": "IPY_MODEL_218feaf18b1d4eb2851dfd3a96902b24",
            "value": " 1.51k/1.51k [00:00&lt;00:00, 195kB/s]"
          }
        },
        "dc1502c68b5f4c6fb36b8806c4913c21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7cebc7ad264417badd837693fd34ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93992552b0f4f1db124fe86a8c00cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e505a627f3c454b8d03665f46b46599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f43eb5f616146c8bd3992ace0fd3b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf2c81a32fd348baa9985e477a4c8467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218feaf18b1d4eb2851dfd3a96902b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a012d8eff847e3928baaa24fd94150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c59f9ca6d23445f84b6ef2a6aad0fb5",
              "IPY_MODEL_15eeefffeee44cccb49d57ba19925484",
              "IPY_MODEL_ba7de3cad0f14abc88c01023f0c22e9b"
            ],
            "layout": "IPY_MODEL_2349a845c8374ac9bd1953e74475e417"
          }
        },
        "6c59f9ca6d23445f84b6ef2a6aad0fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67d4cb5570bb42ada1eec5b99d2279f7",
            "placeholder": "​",
            "style": "IPY_MODEL_483139a905ed4a2b90dcb86357b62acf",
            "value": "vocab.json: 100%"
          }
        },
        "15eeefffeee44cccb49d57ba19925484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08014e714c3740988c2ff0d045349edf",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa35d166f4724b669b149265a669361c",
            "value": 898822
          }
        },
        "ba7de3cad0f14abc88c01023f0c22e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd66e394cf3042feaf46a1fe8ec74cf3",
            "placeholder": "​",
            "style": "IPY_MODEL_47c9e256ca4a45daad9d43dd306da217",
            "value": " 899k/899k [00:00&lt;00:00, 4.05MB/s]"
          }
        },
        "2349a845c8374ac9bd1953e74475e417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67d4cb5570bb42ada1eec5b99d2279f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "483139a905ed4a2b90dcb86357b62acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08014e714c3740988c2ff0d045349edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa35d166f4724b669b149265a669361c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd66e394cf3042feaf46a1fe8ec74cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c9e256ca4a45daad9d43dd306da217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc3f48e39ce848b9bce443258ebf9870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_529fbd9340504482a9df7464ea012e23",
              "IPY_MODEL_4cb7903ddaec48a2ab5901732a12041f",
              "IPY_MODEL_7456a162c55d4877811ec7b1e9bb6868"
            ],
            "layout": "IPY_MODEL_461dcbdee54b406a9b9e5afe0af9a0e9"
          }
        },
        "529fbd9340504482a9df7464ea012e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_044d5fe095894707a3ca671e77a8afe1",
            "placeholder": "​",
            "style": "IPY_MODEL_2705563790444b2fa8fbec4005fb5715",
            "value": "merges.txt: 100%"
          }
        },
        "4cb7903ddaec48a2ab5901732a12041f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65895f7102b64c0783f346628975d515",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb7adc76788142d9b98f2465c13483da",
            "value": 456318
          }
        },
        "7456a162c55d4877811ec7b1e9bb6868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d22ea665461b4ab19a238430033f789d",
            "placeholder": "​",
            "style": "IPY_MODEL_20df82fd6d534e588ae16e3c33d5bf56",
            "value": " 456k/456k [00:00&lt;00:00, 2.12MB/s]"
          }
        },
        "461dcbdee54b406a9b9e5afe0af9a0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044d5fe095894707a3ca671e77a8afe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2705563790444b2fa8fbec4005fb5715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65895f7102b64c0783f346628975d515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7adc76788142d9b98f2465c13483da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d22ea665461b4ab19a238430033f789d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20df82fd6d534e588ae16e3c33d5bf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66b3af92aa5c412da1a602891b194c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a950228e9116451e98c931a77dfa5222",
              "IPY_MODEL_ad459ae01e794e949b5cbf4fee22577b",
              "IPY_MODEL_38a5e22938c048e0948cd43eb38672c1"
            ],
            "layout": "IPY_MODEL_b7affca007c945b98e78703984d1df12"
          }
        },
        "a950228e9116451e98c931a77dfa5222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_672f233548104955bbed97fe0f3f3190",
            "placeholder": "​",
            "style": "IPY_MODEL_10bf16acc0ff45658f4abb212885ea48",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "ad459ae01e794e949b5cbf4fee22577b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19209e4d2d9a4b728f81123568f09b35",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c7cc289d50445c4a849b1ec2d30df58",
            "value": 772
          }
        },
        "38a5e22938c048e0948cd43eb38672c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e4d30dee57a46c29b17559c85ebaed9",
            "placeholder": "​",
            "style": "IPY_MODEL_30b16cc66e9f4865bf1fc12cf2e47add",
            "value": " 772/772 [00:00&lt;00:00, 70.9kB/s]"
          }
        },
        "b7affca007c945b98e78703984d1df12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672f233548104955bbed97fe0f3f3190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bf16acc0ff45658f4abb212885ea48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19209e4d2d9a4b728f81123568f09b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7cc289d50445c4a849b1ec2d30df58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e4d30dee57a46c29b17559c85ebaed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b16cc66e9f4865bf1fc12cf2e47add": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f68ba7786f1460b9f3f1a7539a59ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_119a06555f8f4ea2bd888e1446e4f8c7",
              "IPY_MODEL_4c49a3719db64a178b7ffc119eae5347",
              "IPY_MODEL_8d0c361f31de480685ecd3b3f094969c"
            ],
            "layout": "IPY_MODEL_d0f392371b094d4aafee9727e36e312b"
          }
        },
        "119a06555f8f4ea2bd888e1446e4f8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb6b5a89e02d44289baedc5ee5992636",
            "placeholder": "​",
            "style": "IPY_MODEL_06333001857b4837ad9429ca96203caf",
            "value": "Map: 100%"
          }
        },
        "4c49a3719db64a178b7ffc119eae5347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37d4c06802bf43418382d3a9aceff941",
            "max": 13821,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec63851f8e2440328ff990d1db06ca9b",
            "value": 13821
          }
        },
        "8d0c361f31de480685ecd3b3f094969c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72e78ef5eb7a4e53a2ac9103844ef432",
            "placeholder": "​",
            "style": "IPY_MODEL_38805c7b357042f296c04606c2a6985d",
            "value": " 13821/13821 [00:04&lt;00:00, 2966.10 examples/s]"
          }
        },
        "d0f392371b094d4aafee9727e36e312b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6b5a89e02d44289baedc5ee5992636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06333001857b4837ad9429ca96203caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d4c06802bf43418382d3a9aceff941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec63851f8e2440328ff990d1db06ca9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72e78ef5eb7a4e53a2ac9103844ef432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38805c7b357042f296c04606c2a6985d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fca4a8803fc404bac3e717c95459396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8e931f6588f421d925a9ba851032b89",
              "IPY_MODEL_545ff8cadd26431ab73bda4e1099325b",
              "IPY_MODEL_717a1f0b341e462ebed9a0f8a57d4fbc"
            ],
            "layout": "IPY_MODEL_0be0f486efc2405f9f48232a4a2d5dd1"
          }
        },
        "e8e931f6588f421d925a9ba851032b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847481fa35554ffba7e1e64046b0afbc",
            "placeholder": "​",
            "style": "IPY_MODEL_6d6b9d37264543eeae4d55f16011b215",
            "value": "Map: 100%"
          }
        },
        "545ff8cadd26431ab73bda4e1099325b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80ac199e4ae46c992472b101e57527f",
            "max": 480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aba6210c05fa4942bea0afdfe8154643",
            "value": 480
          }
        },
        "717a1f0b341e462ebed9a0f8a57d4fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b6c9e094104e94840cb44fa9f4600a",
            "placeholder": "​",
            "style": "IPY_MODEL_912d59d9abf64674880618fca7c560fd",
            "value": " 480/480 [00:00&lt;00:00, 4173.69 examples/s]"
          }
        },
        "0be0f486efc2405f9f48232a4a2d5dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847481fa35554ffba7e1e64046b0afbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d6b9d37264543eeae4d55f16011b215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b80ac199e4ae46c992472b101e57527f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aba6210c05fa4942bea0afdfe8154643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39b6c9e094104e94840cb44fa9f4600a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "912d59d9abf64674880618fca7c560fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26210d6821014ca3aaff2ab40e56f621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5264489ac1d548f5ad097f3f3b78dfa0",
              "IPY_MODEL_861d81e9b2b34b818dfe6e9b79c0bb54",
              "IPY_MODEL_570f48474c8b4a7595c328eaabe3e69f"
            ],
            "layout": "IPY_MODEL_298465ded6224bbd9e9f8df7dfe726a6"
          }
        },
        "5264489ac1d548f5ad097f3f3b78dfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ded51998ee465dbfd0c3f084688ef4",
            "placeholder": "​",
            "style": "IPY_MODEL_813d6dbe9be84620b4b4958f34a11c6c",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "861d81e9b2b34b818dfe6e9b79c0bb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5e274568b5c480f9533d911aaf9413f",
            "max": 716280733,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abdee0eba8414468a9db23545c2dff9a",
            "value": 716280733
          }
        },
        "570f48474c8b4a7595c328eaabe3e69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39c3371501f54128958b37013de73950",
            "placeholder": "​",
            "style": "IPY_MODEL_8f6b2fdcaeaa45bc83a407fd64e159d8",
            "value": " 716M/716M [00:02&lt;00:00, 381MB/s]"
          }
        },
        "298465ded6224bbd9e9f8df7dfe726a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19ded51998ee465dbfd0c3f084688ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813d6dbe9be84620b4b4958f34a11c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5e274568b5c480f9533d911aaf9413f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdee0eba8414468a9db23545c2dff9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39c3371501f54128958b37013de73950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6b2fdcaeaa45bc83a407fd64e159d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SNCA-24/5218_dl_snca.py/blob/main/LLM_Distractor_Ranking_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Preparation & Training"
      ],
      "metadata": {
        "id": "i5zZoE2vNsjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QgBDFgzLGEtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c9aabab8-e5de-4429-c76f-4701acdab8b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl.metadata (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.28.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.28.0) (2025.1.31)\n",
            "Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.28.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# cell 1 - dependencies\n",
        "!pip install torch\n",
        "!pip install transformers==4.28.0\n",
        "!pip install datasets scipy matplotlib seaborn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 2 - import libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "\n",
        "from datasets import Dataset\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "BKr4xRNh1PEl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 – configs\n",
        "\n",
        "import os\n",
        "\n",
        "# Test mode: 'smoke' for quick test (low config only), 'full' for all configs\n",
        "TEST_MODE = 'full'\n",
        "\n",
        "# File paths\n",
        "HUMAN_RANKED_PATH = '/content/human_ranked.csv'\n",
        "MMLU_PATH = '/content/training_data.csv'\n",
        "\n",
        "# Model names from Hugging Face\n",
        "MODEL_NAMES = [\n",
        "    'google/t5-efficient-mini',\n",
        "    'google-t5/t5-small',\n",
        "    'google/flan-t5-small',\n",
        "    'sshleifer/distilbart-cnn-6-6',\n",
        "    'sshleifer/distilbart-xsum-12-3',\n",
        "]\n",
        "\n",
        "# Number of labels for classification\n",
        "NUM_LABELS = 4\n",
        "\n",
        "# Hyperparameter settings\n",
        "HYPERPARAMS = {\n",
        "    'low':    {'batch_size': 4,  'epochs': 1, 'learning_rate': 1e-5},\n",
        "    'medium': {'batch_size': 8,  'epochs': 3, 'learning_rate': 5e-5},\n",
        "    'high':   {'batch_size': 16, 'epochs': 5, 'learning_rate': 1e-4},\n",
        "}\n",
        "\n",
        "# Pick which settings to run\n",
        "if TEST_MODE == 'smoke':\n",
        "    HYPERPARAM_SETTINGS = ['low']\n",
        "else:\n",
        "    HYPERPARAM_SETTINGS = list(HYPERPARAMS.keys())\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = './experiments'\n",
        "VISUALIZATIONS_DIR = './plots'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "dGcIRvXw4YIs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 4 - load datasets\n",
        "def load_datasets(human_ranked_path, mmlu_path):\n",
        "    \"\"\"\n",
        "    Load human_ranked and MMLU datasets into pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        human_ranked_path (str): Path to human_ranked.csv\n",
        "        mmlu_path (str): Path to training_data_sample.csv\n",
        "\n",
        "    Returns:\n",
        "        tuple: (human_ranked_df, mmlu_df)\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If input files are not found\n",
        "        ValueError: If expected columns are missing\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Load datasets\n",
        "    human_ranked_df = pd.read_csv(HUMAN_RANKED_PATH)\n",
        "    mmlu_df = pd.read_csv(MMLU_PATH)\n",
        "\n",
        "    # Expected columns\n",
        "    human_ranked_columns = [\n",
        "        'subject', 'question', 'correct_answer', 'option_0', 'option_1',\n",
        "        'option_2', 'option_3', 'distractor_ranking_best_to_worst_Annotator_1',\n",
        "        'distractor_ranking_best_to_worst_Annotator_2'\n",
        "    ]\n",
        "\n",
        "    mmlu_columns = ['question', 'subject', 'choices', 'correct_answer', 'option_0', 'option_1', 'option_2', 'option_3']\n",
        "\n",
        "    # Validate columns\n",
        "    if not all(col in human_ranked_df.columns for col in human_ranked_columns):\n",
        "        missing = [col for col in human_ranked_columns if col not in human_ranked_df.columns]\n",
        "        raise ValueError(f\"Missing columns in human_ranked_df: {missing}\")\n",
        "    if not all(col in mmlu_df.columns for col in mmlu_columns):\n",
        "        missing = [col for col in mmlu_columns if col not in mmlu_df.columns]\n",
        "        raise ValueError(f\"Missing columns in mmlu_df: {missing}\")\n",
        "\n",
        "    return human_ranked_df, mmlu_df\n",
        "\n",
        "# Load datasets using constants from Cell 3\n",
        "human_ranked_df, mmlu_df = load_datasets(HUMAN_RANKED_PATH, MMLU_PATH)\n",
        "\n"
      ],
      "metadata": {
        "id": "1-UAZwRx4wqo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 - Preprocess Data\n",
        "\n",
        "def preprocess_data(human_ranked_df, mmlu_df):\n",
        "    # Print mmlu_df columns to diagnose KeyError\n",
        "    print(\"mmlu_df columns:\", mmlu_df.columns.tolist())\n",
        "\n",
        "    # Identify rows to keep and drop for human_ranked_df\n",
        "    critical_columns_human = [\n",
        "        'subject', 'question', 'correct_answer',\n",
        "        'option_0', 'option_1', 'option_2', 'option_3',\n",
        "        'distractor_ranking_best_to_worst_Annotator_1'\n",
        "    ]\n",
        "    keep_mask_human = human_ranked_df[critical_columns_human].notna().all(axis=1)\n",
        "    human_ranked_df_dropped = human_ranked_df.loc[~keep_mask_human].copy()\n",
        "    human_ranked_df_keep = human_ranked_df.loc[keep_mask_human].copy()\n",
        "\n",
        "    # Log dropped rows for human_ranked_df\n",
        "    dropped_count_human = len(human_ranked_df_dropped)\n",
        "    print(f\"Dropped {dropped_count_human} rows from human_ranked_df. Saved to ./dropped_human_ranked_rows.csv\")\n",
        "    human_ranked_df_dropped.to_csv('./dropped_human_ranked_rows.csv', index=False)\n",
        "\n",
        "    # Identify rows to keep and drop for mmlu_df\n",
        "    critical_columns_mmlu = [\n",
        "        'subject', 'question', 'correct_answer',\n",
        "        'option_0', 'option_1', 'option_2', 'option_3'\n",
        "    ]\n",
        "    # Check if critical columns exist in mmlu_df\n",
        "    missing_columns = [col for col in critical_columns_mmlu if col not in mmlu_df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Warning: Columns {missing_columns} not found in mmlu_df. Available columns: {mmlu_df.columns.tolist()}\")\n",
        "        # Proceed with available columns only\n",
        "        critical_columns_mmlu = [col for col in critical_columns_mmlu if col in mmlu_df.columns]\n",
        "\n",
        "    keep_mask_mmlu = mmlu_df[critical_columns_mmlu].notna().all(axis=1)\n",
        "    mmlu_df_dropped = mmlu_df.loc[~keep_mask_mmlu].copy()\n",
        "    mmlu_df_keep = mmlu_df.loc[keep_mask_mmlu].copy()\n",
        "\n",
        "    # Log dropped rows for mmlu_df\n",
        "    dropped_count_mmlu = len(mmlu_df_dropped)\n",
        "    print(f\"Dropped {dropped_count_mmlu} rows from mmlu_df. Saved to ./dropped_mmlu_rows.csv\")\n",
        "    mmlu_df_dropped.to_csv('./dropped_mmlu_rows.csv', index=False)\n",
        "\n",
        "    # (Removed standardization of option columns - no renaming performed)\n",
        "\n",
        "    # Verify that human_ranked_df_keep has required option columns\n",
        "    required_options = ['option_0', 'option_1', 'option_2', 'option_3']\n",
        "    missing_options = [opt for opt in required_options if opt not in human_ranked_df_keep.columns]\n",
        "    if missing_options:\n",
        "        print(f\"Error: Required option columns {missing_options} missing in human_ranked_df.\")\n",
        "\n",
        "    return human_ranked_df_keep, mmlu_df_keep\n",
        "\n",
        "# Apply preprocessing\n",
        "human_ranked_df, mmlu_df = preprocess_data(human_ranked_df, mmlu_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bexYBh-BPXi",
        "outputId": "b67b35f1-3b06-430a-ffaf-958f765aad18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mmlu_df columns: ['question', 'subject', 'choices', 'correct_answer', 'option_0', 'option_1', 'option_2', 'option_3']\n",
            "Dropped 0 rows from human_ranked_df. Saved to ./dropped_human_ranked_rows.csv\n",
            "Dropped 10 rows from mmlu_df. Saved to ./dropped_mmlu_rows.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_ranked_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "s-amlLkdtDHe",
        "outputId": "10c70fef-e045-4b55-b319-c4693602eb6b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      subject  question_id  \\\n",
              "0  high_school_microeconomics            1   \n",
              "1  high_school_microeconomics            2   \n",
              "2  high_school_microeconomics            3   \n",
              "3  high_school_microeconomics            4   \n",
              "4  high_school_microeconomics            5   \n",
              "\n",
              "                                            question  correct_answer  \\\n",
              "0  Which of the following is not a reason why dem...               1   \n",
              "1  At the profit-maximizing quantity, a perfectly...               1   \n",
              "2               A negative externality results in...               0   \n",
              "3     Which statement is false about price ceilings?               2   \n",
              "4  Cross-price elasticity of demand between two g...               1   \n",
              "\n",
              "                                            option_0  \\\n",
              "0  An increase in consumer income (for normal goods)   \n",
              "1                                   Marginal revenue   \n",
              "2                    Overproduction and underpricing   \n",
              "3                         They can lead to shortages   \n",
              "4                          The goods are substitutes   \n",
              "\n",
              "                                   option_1  \\\n",
              "0    A fall in the price of the good itself   \n",
              "1                             Marginal cost   \n",
              "2           Underproduction and overpricing   \n",
              "3  They are set below the equilibrium price   \n",
              "4                 The goods are complements   \n",
              "\n",
              "                                option_2  \\\n",
              "0    A rise in the price of a substitute   \n",
              "1                           Average cost   \n",
              "2                Efficient market output   \n",
              "3  They always increase producer surplus   \n",
              "4                     They are unrelated   \n",
              "\n",
              "                                option_3  \\\n",
              "0  An expected increase in future prices   \n",
              "1                       None of the them   \n",
              "2                       None of the them   \n",
              "3         They distort market efficiency   \n",
              "4                   Cannot be determined   \n",
              "\n",
              "  distractor_ranking_best_to_worst_Annotator_1  \\\n",
              "0                                        2,3,0   \n",
              "1                                        0,2,3   \n",
              "2                                        1,3,2   \n",
              "3                                        0,1,3   \n",
              "4                                        0,2,3   \n",
              "\n",
              "  distractor_ranking_best_to_worst_Annotator_2  \n",
              "0                                          NaN  \n",
              "1                                          NaN  \n",
              "2                                          NaN  \n",
              "3                                          NaN  \n",
              "4                                          NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52fc8248-1f3e-4e1a-8140-38302198fad0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>correct_answer</th>\n",
              "      <th>option_0</th>\n",
              "      <th>option_1</th>\n",
              "      <th>option_2</th>\n",
              "      <th>option_3</th>\n",
              "      <th>distractor_ranking_best_to_worst_Annotator_1</th>\n",
              "      <th>distractor_ranking_best_to_worst_Annotator_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>high_school_microeconomics</td>\n",
              "      <td>1</td>\n",
              "      <td>Which of the following is not a reason why dem...</td>\n",
              "      <td>1</td>\n",
              "      <td>An increase in consumer income (for normal goods)</td>\n",
              "      <td>A fall in the price of the good itself</td>\n",
              "      <td>A rise in the price of a substitute</td>\n",
              "      <td>An expected increase in future prices</td>\n",
              "      <td>2,3,0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>high_school_microeconomics</td>\n",
              "      <td>2</td>\n",
              "      <td>At the profit-maximizing quantity, a perfectly...</td>\n",
              "      <td>1</td>\n",
              "      <td>Marginal revenue</td>\n",
              "      <td>Marginal cost</td>\n",
              "      <td>Average cost</td>\n",
              "      <td>None of the them</td>\n",
              "      <td>0,2,3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>high_school_microeconomics</td>\n",
              "      <td>3</td>\n",
              "      <td>A negative externality results in...</td>\n",
              "      <td>0</td>\n",
              "      <td>Overproduction and underpricing</td>\n",
              "      <td>Underproduction and overpricing</td>\n",
              "      <td>Efficient market output</td>\n",
              "      <td>None of the them</td>\n",
              "      <td>1,3,2</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>high_school_microeconomics</td>\n",
              "      <td>4</td>\n",
              "      <td>Which statement is false about price ceilings?</td>\n",
              "      <td>2</td>\n",
              "      <td>They can lead to shortages</td>\n",
              "      <td>They are set below the equilibrium price</td>\n",
              "      <td>They always increase producer surplus</td>\n",
              "      <td>They distort market efficiency</td>\n",
              "      <td>0,1,3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>high_school_microeconomics</td>\n",
              "      <td>5</td>\n",
              "      <td>Cross-price elasticity of demand between two g...</td>\n",
              "      <td>1</td>\n",
              "      <td>The goods are substitutes</td>\n",
              "      <td>The goods are complements</td>\n",
              "      <td>They are unrelated</td>\n",
              "      <td>Cannot be determined</td>\n",
              "      <td>0,2,3</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52fc8248-1f3e-4e1a-8140-38302198fad0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-52fc8248-1f3e-4e1a-8140-38302198fad0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-52fc8248-1f3e-4e1a-8140-38302198fad0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ec63b5d2-638c-4afa-8079-c8cbb94287e4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec63b5d2-638c-4afa-8079-c8cbb94287e4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ec63b5d2-638c-4afa-8079-c8cbb94287e4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "human_ranked_df",
              "summary": "{\n  \"name\": \"human_ranked_df\",\n  \"rows\": 480,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"high_school_microeconomics\",\n          \"global_facts\",\n          \"formal_logic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 138,\n        \"min\": 1,\n        \"max\": 480,\n        \"num_unique_values\": 480,\n        \"samples\": [\n          74,\n          415,\n          395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 477,\n        \"samples\": [\n          \"Which method is used for hyperparameter tuning in machine learning?\",\n          \"In 2021, what percentage of global electricity came from renewable sources?\",\n          \"A hash function guarantees the integrity of a message. It guarantees that the message has not be\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_answer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_0\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 428,\n        \"samples\": [\n          \"To measure how well the model is performing\",\n          \"Invasions by barbarian tribes\",\n          \"I only\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 430,\n        \"samples\": [\n          \"Support Vector Machine\",\n          \"Nazi attempts to seize control of East Germany\",\n          \"The 100-W bulb is twice as bright.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 439,\n        \"samples\": [\n          \"Limited dispersal abilities\",\n          \"An outbreak of civil war in Germany\",\n          \"an alcohol and an acid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 432,\n        \"samples\": [\n          \"To prevent overfitting\",\n          \"Simon Bolivar\",\n          \"The bar cannot accelerate translationally or rotationally.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"distractor_ranking_best_to_worst_Annotator_1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 25,\n        \"samples\": [\n          \"1,0,3\",\n          \"3,2,1\",\n          \"2,3,0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"distractor_ranking_best_to_worst_Annotator_2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 23,\n        \"samples\": [\n          \"3,1,0\",\n          \"3,1,2\",\n          \"1,0,3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmlu_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "be3ZMyPAtMqK",
        "outputId": "b6c50990-8d6a-41e6-ef94-b51dbaaa37bd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question           subject  \\\n",
              "0  Find the degree for the given field extension ...  abstract_algebra   \n",
              "1  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...  abstract_algebra   \n",
              "2  Find all zeros in the indicated finite field o...  abstract_algebra   \n",
              "3  Statement 1 | A factor group of a non-Abelian ...  abstract_algebra   \n",
              "4  Find the product of the given polynomials in t...  abstract_algebra   \n",
              "\n",
              "                                             choices  correct_answer  \\\n",
              "0                                  ['0' '4' '2' '6']               1   \n",
              "1                               ['8' '2' '24' '120']               2   \n",
              "2                              ['0' '1' '0,1' '0,4']               3   \n",
              "3  ['True, True' 'False, False' 'True, False' 'Fa...               1   \n",
              "4         ['2x^2 + 5' '6x^2 + 4x + 6' '0' 'x^2 + 1']               1   \n",
              "\n",
              "     option_0       option_1     option_2     option_3  \n",
              "0           0              4            2            6  \n",
              "1           8              2           24          120  \n",
              "2           0              1          0,1          0,4  \n",
              "3  True, True   False, False  True, False  False, True  \n",
              "4    2x^2 + 5  6x^2 + 4x + 6            0      x^2 + 1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8519b719-31d2-4546-b983-8764361d9cd5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>choices</th>\n",
              "      <th>correct_answer</th>\n",
              "      <th>option_0</th>\n",
              "      <th>option_1</th>\n",
              "      <th>option_2</th>\n",
              "      <th>option_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Find the degree for the given field extension ...</td>\n",
              "      <td>abstract_algebra</td>\n",
              "      <td>['0' '4' '2' '6']</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...</td>\n",
              "      <td>abstract_algebra</td>\n",
              "      <td>['8' '2' '24' '120']</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Find all zeros in the indicated finite field o...</td>\n",
              "      <td>abstract_algebra</td>\n",
              "      <td>['0' '1' '0,1' '0,4']</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0,1</td>\n",
              "      <td>0,4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Statement 1 | A factor group of a non-Abelian ...</td>\n",
              "      <td>abstract_algebra</td>\n",
              "      <td>['True, True' 'False, False' 'True, False' 'Fa...</td>\n",
              "      <td>1</td>\n",
              "      <td>True, True</td>\n",
              "      <td>False, False</td>\n",
              "      <td>True, False</td>\n",
              "      <td>False, True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Find the product of the given polynomials in t...</td>\n",
              "      <td>abstract_algebra</td>\n",
              "      <td>['2x^2 + 5' '6x^2 + 4x + 6' '0' 'x^2 + 1']</td>\n",
              "      <td>1</td>\n",
              "      <td>2x^2 + 5</td>\n",
              "      <td>6x^2 + 4x + 6</td>\n",
              "      <td>0</td>\n",
              "      <td>x^2 + 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8519b719-31d2-4546-b983-8764361d9cd5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8519b719-31d2-4546-b983-8764361d9cd5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8519b719-31d2-4546-b983-8764361d9cd5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ee61d97f-ebad-42b2-ac5d-5e6b26f7ce39\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee61d97f-ebad-42b2-ac5d-5e6b26f7ce39')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ee61d97f-ebad-42b2-ac5d-5e6b26f7ce39 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mmlu_df",
              "summary": "{\n  \"name\": \"mmlu_df\",\n  \"rows\": 13821,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13662,\n        \"samples\": [\n          \"A machine learning problem involves four attributes plus a class. The attributes have 3, 2, 2, and 2 possible values each. The class has 3 possible values. How many maximum possible different examples are there?\",\n          \"A confidence interval estimate is determined from the monthly grocery expenditures in a random sample of n families. Which of the following will result in a smaller margin of error? I. A smaller confidence level. II. A smaller sample standard deviation. III. A smaller sample size\",\n          \" According to Sandel, eugenics and genetic enhancement can be seen to be wrong from a secular perspective because they would produce unwelcome changes in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 57,\n        \"samples\": [\n          \"abstract_algebra\",\n          \"college_biology\",\n          \"high_school_us_history\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"choices\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12505,\n        \"samples\": [\n          \"['0.0495' '0.078' '0.635' '0.97']\",\n          \"['The contract clause prohibition against a state from enacting any law that will impair the obligation of contracts.'\\n 'The privileges and immunities clause of the Fourteenth Amendment.'\\n 'The privileges and immunities clause under Article IV, Section 2.'\\n 'The national property power provision under Article IV, Section 3.']\",\n          \"['nothing.' '$10,000. 00'\\n '$10,000, only if the debtor is successful in suing the person who had contracted to buy his house.'\\n '$15,000. 00']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_answer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_0\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11674,\n        \"samples\": [\n          \"It specifies the poverty line at a level set in the 1960s and adjusted since to reflect inflation.\",\n          \"0.05\",\n          \"Electrons flow down an electron transport chain as they are attracted to oxygen.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11767,\n        \"samples\": [\n          \"organizational analysis\",\n          \"Selling.\",\n          \"Epistemic violence.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11763,\n        \"samples\": [\n          \"An increase in the price of tuba lessons\",\n          \"Government IT systems\",\n          \"The mitochondria of a plant and those of an animal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"option_3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11389,\n        \"samples\": [\n          \"The firm will realize zero economic profits.\",\n          \"13.9\",\n          \"none of these.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction Method Finalised - Log Probabibilty Scoring"
      ],
      "metadata": {
        "id": "OJ7rQTbBqT0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 – Distractor Analysis Class (add log-prob scoring)\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "\n",
        "class DistractorAnalysis:\n",
        "    def __init__(self, model_name, hyperparam_setting):\n",
        "        self.model_name = model_name\n",
        "        self.hyperparam_setting = hyperparam_setting\n",
        "\n",
        "        if hyperparam_setting not in HYPERPARAMS:\n",
        "            raise ValueError(f\"Invalid hyperparam_setting: {hyperparam_setting}\")\n",
        "\n",
        "        # Load full encoder–decoder LM\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model     = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "        # TrainingArguments stay the same as before\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(\n",
        "                OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{hyperparam_setting}\"\n",
        "            ),\n",
        "            per_device_train_batch_size=HYPERPARAMS[hyperparam_setting]['batch_size'],\n",
        "            per_device_eval_batch_size= HYPERPARAMS[hyperparam_setting]['batch_size'],\n",
        "            num_train_epochs=         HYPERPARAMS[hyperparam_setting]['epochs'],\n",
        "            learning_rate=            HYPERPARAMS[hyperparam_setting]['learning_rate'],\n",
        "            evaluation_strategy=      \"steps\",\n",
        "            eval_steps=               1000,\n",
        "            logging_steps=            500,\n",
        "            save_steps=               1000,\n",
        "            save_total_limit=         2,\n",
        "            load_best_model_at_end=   True,\n",
        "            metric_for_best_model=    \"loss\",  # or \"eval_loss\"\n",
        "            greater_is_better=        False,\n",
        "            remove_unused_columns=    False,\n",
        "            logging_dir=os.path.join(\n",
        "                OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{hyperparam_setting}_logs\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def train(self, tokenized_datasets):\n",
        "        \"\"\"\n",
        "        Fine‐tune the seq2seq LM in a standard teacher‐forcing way,\n",
        "        where the target is the single letter token (A/B/C/D).\n",
        "        \"\"\"\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=tokenized_datasets[self.model_name]['train'],\n",
        "            eval_dataset= tokenized_datasets[self.model_name]['eval'],\n",
        "        )\n",
        "        trainer.train()\n",
        "        ckpt = os.path.join(\n",
        "            OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}_best\"\n",
        "        )\n",
        "        trainer.save_model(ckpt)\n",
        "        self.tokenizer.save_pretrained(ckpt)\n",
        "\n",
        "    def predict_logprob(self, dataset, batch_size=1, device='cuda'):\n",
        "        \"\"\"\n",
        "        For each input_text, score each letter token (A/B/C/D) by the\n",
        "        total log‐prob of generating that letter next, then pick the best.\n",
        "        Returns:\n",
        "          preds_arr: np.array of shape [N] with 0–3\n",
        "          logprobs:  np.array of shape [N,4] with the log‐prob for each letter\n",
        "        \"\"\"\n",
        "        dev = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(dev).eval()\n",
        "\n",
        "        # map letter idx → string\n",
        "        letter_str = [\"A\",\"B\",\"C\",\"D\"]\n",
        "        N = len(dataset)\n",
        "        all_preds    = []\n",
        "        all_logprobs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for ex in dataset:\n",
        "                prompt = ex['input_text']\n",
        "                # tokenize prompt once\n",
        "                enc = self.tokenizer(prompt, return_tensors='pt').to(dev)\n",
        "                input_ids = enc.input_ids\n",
        "                attn_mask = enc.attention_mask\n",
        "\n",
        "                # score each letter\n",
        "                letter_scores = []\n",
        "                for L in letter_str:\n",
        "                    # teacher‐force L as next token\n",
        "                    lab_ids = self.tokenizer(L, add_special_tokens=False, return_tensors='pt').input_ids.to(dev)\n",
        "                    full_ids   = torch.cat([input_ids, lab_ids], dim=-1)\n",
        "                    full_mask  = torch.cat([attn_mask, torch.ones_like(lab_ids)], dim=-1)\n",
        "\n",
        "                    out = self.model(\n",
        "                        input_ids=full_ids,\n",
        "                        attention_mask=full_mask,\n",
        "                        labels=full_ids\n",
        "                    )\n",
        "                    # out.loss is average NLL per token → multiply by #tokens\n",
        "                    total_nll = out.loss.item() * lab_ids.size(1)\n",
        "                    letter_scores.append(-total_nll)\n",
        "\n",
        "                # pick best\n",
        "                scores_arr = np.array(letter_scores)\n",
        "                best_idx   = int(scores_arr.argmax())\n",
        "                all_preds.append(best_idx)\n",
        "                all_logprobs.append(scores_arr)\n",
        "\n",
        "        return np.array(all_preds), np.stack(all_logprobs, axis=0)"
      ],
      "metadata": {
        "id": "Fep2vWkiqX9R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - Prepare Datasets for Training\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "def prepare_datasets(human_ranked_df, mmlu_df):\n",
        "    # Verify required global variables\n",
        "    if 'MODEL_NAMES' not in globals():\n",
        "        raise NameError(\"MODEL_NAMES global variable not defined. Please run Cell #3 first.\")\n",
        "\n",
        "    option_columns = ['option_0', 'option_1', 'option_2', 'option_3']\n",
        "\n",
        "    # Map answer index to choice token (A, B, C, D)\n",
        "    def map_answer_to_choice(row, answer_col):\n",
        "        try:\n",
        "            answer_idx = int(row[answer_col])\n",
        "            if answer_idx not in [0, 1, 2, 3]:\n",
        "                return None\n",
        "            return chr(65 + answer_idx)  # 0 -> 'A', 1 -> 'B', 2 -> 'C', 3 -> 'D'\n",
        "        except (ValueError, TypeError, KeyError):\n",
        "            return None\n",
        "\n",
        "    # Validate columns in mmlu_df\n",
        "    missing_mmlu_cols = [col for col in ['question', 'correct_answer'] + option_columns if col not in mmlu_df.columns]\n",
        "    if missing_mmlu_cols:\n",
        "        print(f\"Warning: Missing columns in mmlu_df: {missing_mmlu_cols}. Available columns: {mmlu_df.columns.tolist()}\")\n",
        "        fallback_cols = ['choice1', 'choice2', 'choice3', 'choice4']\n",
        "        if all(col in mmlu_df.columns for col in fallback_cols):\n",
        "            print(\"Falling back to choice1, choice2, choice3, choice4 for mmlu_df\")\n",
        "            mmlu_df = mmlu_df.rename(columns={\n",
        "                'choice1': 'option_0',\n",
        "                'choice2': 'option_1',\n",
        "                'choice3': 'option_2',\n",
        "                'choice4': 'option_3'\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(\"Cannot proceed: mmlu_df missing required option columns\")\n",
        "\n",
        "    # Validate columns in human_ranked_df - using consistent approach with mmlu_df\n",
        "    missing_human_cols = [col for col in ['question', 'correct_answer'] + option_columns if col not in human_ranked_df.columns]\n",
        "    if missing_human_cols:\n",
        "        print(f\"Warning: Missing columns in human_ranked_df: {missing_human_cols}. Available columns: {human_ranked_df.columns.tolist()}\")\n",
        "        # Could add fallback logic here similar to mmlu_df if needed\n",
        "        raise ValueError(\"Cannot proceed: human_ranked_df missing required columns\")\n",
        "\n",
        "    # Process mmlu_df with safer type conversion\n",
        "    mmlu_df = mmlu_df.copy()\n",
        "    mmlu_df['correct_choice'] = mmlu_df.apply(lambda row: map_answer_to_choice(row, 'correct_answer'), axis=1)\n",
        "\n",
        "    # Safer conversion with validation\n",
        "    mmlu_df['correct_index'] = mmlu_df['correct_answer'].apply(\n",
        "        lambda x: int(x) if pd.notna(x) and str(x).strip().isdigit() else None\n",
        "    )\n",
        "\n",
        "    mmlu_df_dropped = mmlu_df.loc[(mmlu_df['correct_choice'].isna()) | (mmlu_df['correct_index'].isna())].copy()\n",
        "    mmlu_df = mmlu_df.loc[(mmlu_df['correct_choice'].notna()) & (mmlu_df['correct_index'].notna())].copy()\n",
        "    if len(mmlu_df_dropped) > 0:\n",
        "        print(f\"Dropped {len(mmlu_df_dropped)} rows from mmlu_df. Saved to ./dropped_mmlu_choice_rows.csv\")\n",
        "        mmlu_df_dropped.to_csv('./dropped_mmlu_choice_rows.csv', index=False)\n",
        "\n",
        "    # Process human_ranked_df with safer type conversion\n",
        "    human_ranked_df = human_ranked_df.copy()\n",
        "    human_ranked_df['correct_choice'] = human_ranked_df.apply(lambda row: map_answer_to_choice(row, 'correct_answer'), axis=1)\n",
        "\n",
        "    # Safer conversion with validation\n",
        "    human_ranked_df['correct_index'] = human_ranked_df['correct_answer'].apply(\n",
        "        lambda x: int(x) if pd.notna(x) and str(x).strip().isdigit() else None\n",
        "    )\n",
        "\n",
        "    human_ranked_df_dropped = human_ranked_df.loc[(human_ranked_df['correct_choice'].isna()) | (human_ranked_df['correct_index'].isna())].copy()\n",
        "    human_ranked_df = human_ranked_df.loc[(human_ranked_df['correct_choice'].notna()) & (human_ranked_df['correct_index'].notna())].copy()\n",
        "    if len(human_ranked_df_dropped) > 0:\n",
        "        print(f\"Dropped {len(human_ranked_df_dropped)} rows from human_ranked_df. Saved to ./dropped_human_ranked_choice_rows.csv\")\n",
        "        human_ranked_df_dropped.to_csv('./dropped_human_ranked_choice_rows.csv', index=False)\n",
        "\n",
        "    # Create input text\n",
        "    def create_input_text(row):\n",
        "        return f\"Question: {row['question']} Options: A: {row['option_0']} B: {row['option_1']} C: {row['option_2']} D: {row['option_3']}\"\n",
        "\n",
        "    mmlu_df['input_text'] = mmlu_df.apply(create_input_text, axis=1)\n",
        "    human_ranked_df['input_text'] = human_ranked_df.apply(create_input_text, axis=1)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_pandas(mmlu_df[['input_text', 'correct_choice', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'correct_index']])\n",
        "    eval_dataset = Dataset.from_pandas(human_ranked_df[['input_text', 'correct_choice', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'correct_index']])\n",
        "\n",
        "    # Remove pandas index column if present\n",
        "    for dataset in [train_dataset, eval_dataset]:\n",
        "        if '__index_level_0__' in dataset.column_names:\n",
        "            dataset = dataset.remove_columns('__index_level_0__')\n",
        "\n",
        "    # Tokenization\n",
        "    tokenized_datasets = {}\n",
        "    for model_name in MODEL_NAMES:\n",
        "        try:\n",
        "            print(f\"Tokenizing datasets for model: {model_name}\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            def tokenize_function(examples):\n",
        "                model_inputs = tokenizer(\n",
        "                    examples['input_text'],\n",
        "                    max_length=512,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    return_tensors=None\n",
        "                )\n",
        "                # Tokenize correct_choice as labels\n",
        "                labels = tokenizer(examples['correct_choice'], add_special_tokens=False).input_ids\n",
        "                model_inputs['labels'] = [label[0] if len(label) == 1 else tokenizer.unk_token_id for label in labels]\n",
        "                # Set decoder_input_ids for seq2seq models\n",
        "                start_token = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "                model_inputs['decoder_input_ids'] = [[start_token]] * len(examples['input_text'])\n",
        "                return model_inputs\n",
        "\n",
        "            # Updated code to be more selective about which columns to keep\n",
        "            tokenized_train = train_dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                # Only keep columns needed by the model\n",
        "                remove_columns=[col for col in train_dataset.column_names\n",
        "                               if col not in ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']]\n",
        "            )\n",
        "            tokenized_eval = eval_dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                # Only keep columns needed by the model\n",
        "                remove_columns=[col for col in eval_dataset.column_names\n",
        "                               if col not in ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']]\n",
        "            )\n",
        "\n",
        "            # Verify the tokenized datasets have the required fields for DistractorAnalysis.predict\n",
        "            required_fields = ['input_ids', 'attention_mask', 'labels']\n",
        "            missing_train = [f for f in required_fields if f not in tokenized_train.column_names]\n",
        "            missing_eval = [f for f in required_fields if f not in tokenized_eval.column_names]\n",
        "\n",
        "            if missing_train or missing_eval:\n",
        "                print(f\"Warning: Missing fields for {model_name}. Train: {missing_train}, Eval: {missing_eval}\")\n",
        "                if missing_train or missing_eval:\n",
        "                    raise ValueError(f\"Tokenized datasets missing required fields for model {model_name}\")\n",
        "\n",
        "            tokenized_datasets[model_name] = {'train': tokenized_train, 'eval': tokenized_eval}\n",
        "            print(f\"Tokenized datasets for {model_name}: train={len(tokenized_train)} rows, eval={len(tokenized_eval)} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to tokenize for {model_name}: {str(e)}\")\n",
        "            logging.error(f\"Failed to tokenize for {model_name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Log sizes for sanity check\n",
        "    print(f\"Original sizes - MMLU: {len(mmlu_df)}, Human Ranked: {len(human_ranked_df)}\")\n",
        "    print(f\"After processing - Train dataset: {len(train_dataset)}, Eval dataset: {len(eval_dataset)}\")\n",
        "\n",
        "    return tokenized_datasets, train_dataset, eval_dataset\n",
        "\n",
        "# Execute\n",
        "try:\n",
        "    tokenized_datasets, train_dataset, eval_dataset = prepare_datasets(human_ranked_df, mmlu_df)\n",
        "    print(\"Datasets prepared successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preparing datasets: {str(e)}\")\n",
        "    logging.error(f\"Error preparing datasets: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "818d700256a54a0c9d9c32416499970e",
            "13450e5bf95c444896ef3c6b83cd0227",
            "c7322645efdd4538b6770e64943ced50",
            "1fbd674945dd4fa9a224424abf37b4a7",
            "892eee6cd07f4e819e8c425877e78210",
            "839ad0282a1c489e8d132a13d8b8da43",
            "f224e2e049f749d3a3930b5d68f83f2c",
            "3390a2b01af749f4b41e6a0b2a6814be",
            "c8556d4475154664b88e200188781247",
            "8518126c084644d99e920708828d58a0",
            "9372b26df9fe42599c9f6e7d46796c1c",
            "92e35c909dd040e89346827385311e13",
            "1f10b123357748df8334b0d0aae3f176",
            "2ae5c339ccae43d084c0760682fda056",
            "39c3c2dd9b804ebba245bbc6ec3178b7",
            "dc1502c68b5f4c6fb36b8806c4913c21",
            "cc7cebc7ad264417badd837693fd34ee",
            "a93992552b0f4f1db124fe86a8c00cf9",
            "0e505a627f3c454b8d03665f46b46599",
            "2f43eb5f616146c8bd3992ace0fd3b47",
            "cf2c81a32fd348baa9985e477a4c8467",
            "218feaf18b1d4eb2851dfd3a96902b24",
            "26a012d8eff847e3928baaa24fd94150",
            "6c59f9ca6d23445f84b6ef2a6aad0fb5",
            "15eeefffeee44cccb49d57ba19925484",
            "ba7de3cad0f14abc88c01023f0c22e9b",
            "2349a845c8374ac9bd1953e74475e417",
            "67d4cb5570bb42ada1eec5b99d2279f7",
            "483139a905ed4a2b90dcb86357b62acf",
            "08014e714c3740988c2ff0d045349edf",
            "fa35d166f4724b669b149265a669361c",
            "dd66e394cf3042feaf46a1fe8ec74cf3",
            "47c9e256ca4a45daad9d43dd306da217",
            "dc3f48e39ce848b9bce443258ebf9870",
            "529fbd9340504482a9df7464ea012e23",
            "4cb7903ddaec48a2ab5901732a12041f",
            "7456a162c55d4877811ec7b1e9bb6868",
            "461dcbdee54b406a9b9e5afe0af9a0e9",
            "044d5fe095894707a3ca671e77a8afe1",
            "2705563790444b2fa8fbec4005fb5715",
            "65895f7102b64c0783f346628975d515",
            "bb7adc76788142d9b98f2465c13483da",
            "d22ea665461b4ab19a238430033f789d",
            "20df82fd6d534e588ae16e3c33d5bf56",
            "66b3af92aa5c412da1a602891b194c3e",
            "a950228e9116451e98c931a77dfa5222",
            "ad459ae01e794e949b5cbf4fee22577b",
            "38a5e22938c048e0948cd43eb38672c1",
            "b7affca007c945b98e78703984d1df12",
            "672f233548104955bbed97fe0f3f3190",
            "10bf16acc0ff45658f4abb212885ea48",
            "19209e4d2d9a4b728f81123568f09b35",
            "3c7cc289d50445c4a849b1ec2d30df58",
            "1e4d30dee57a46c29b17559c85ebaed9",
            "30b16cc66e9f4865bf1fc12cf2e47add",
            "6f68ba7786f1460b9f3f1a7539a59ec7",
            "119a06555f8f4ea2bd888e1446e4f8c7",
            "4c49a3719db64a178b7ffc119eae5347",
            "8d0c361f31de480685ecd3b3f094969c",
            "d0f392371b094d4aafee9727e36e312b",
            "eb6b5a89e02d44289baedc5ee5992636",
            "06333001857b4837ad9429ca96203caf",
            "37d4c06802bf43418382d3a9aceff941",
            "ec63851f8e2440328ff990d1db06ca9b",
            "72e78ef5eb7a4e53a2ac9103844ef432",
            "38805c7b357042f296c04606c2a6985d",
            "9fca4a8803fc404bac3e717c95459396",
            "e8e931f6588f421d925a9ba851032b89",
            "545ff8cadd26431ab73bda4e1099325b",
            "717a1f0b341e462ebed9a0f8a57d4fbc",
            "0be0f486efc2405f9f48232a4a2d5dd1",
            "847481fa35554ffba7e1e64046b0afbc",
            "6d6b9d37264543eeae4d55f16011b215",
            "b80ac199e4ae46c992472b101e57527f",
            "aba6210c05fa4942bea0afdfe8154643",
            "39b6c9e094104e94840cb44fa9f4600a",
            "912d59d9abf64674880618fca7c560fd"
          ]
        },
        "id": "o9SrAwpjqdAT",
        "outputId": "793385d9-db43-4b52-d9da-b7483a2ab0b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets for model: sshleifer/distilbart-xsum-12-3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "818d700256a54a0c9d9c32416499970e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92e35c909dd040e89346827385311e13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26a012d8eff847e3928baaa24fd94150"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc3f48e39ce848b9bce443258ebf9870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66b3af92aa5c412da1a602891b194c3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13821 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f68ba7786f1460b9f3f1a7539a59ec7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fca4a8803fc404bac3e717c95459396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized datasets for sshleifer/distilbart-xsum-12-3: train=13821 rows, eval=480 rows\n",
            "Original sizes - MMLU: 13821, Human Ranked: 480\n",
            "After processing - Train dataset: 13821, Eval dataset: 480\n",
            "Datasets prepared successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - Train Models\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def train_all_models(tokenized_datasets, train_dataset, eval_dataset):\n",
        "    # set up logging to file\n",
        "    log_file = os.path.join(OUTPUT_DIR, 'training_log.txt')\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # start message\n",
        "    print(f\"Starting train_all_models with {len(MODEL_NAMES)} models × {len(HYPERPARAMS)} hyperparam settings\")\n",
        "    logging.info(f\"MODEL_NAMES={MODEL_NAMES}, HYPERPARAMS={list(HYPERPARAMS.keys())}\")\n",
        "\n",
        "    # dataset size checks\n",
        "    print(f\"Raw train_dataset size: {len(train_dataset)} rows\")\n",
        "    print(f\"Raw eval_dataset size: {len(eval_dataset)} rows\")\n",
        "    logging.info(f\"Raw train_dataset size: {len(train_dataset)} rows\")\n",
        "    logging.info(f\"Raw eval_dataset size: {len(eval_dataset)} rows\")\n",
        "\n",
        "    if len(train_dataset) == 0:\n",
        "        logging.error(\"train_dataset is empty.\")\n",
        "        raise ValueError(\"train_dataset is empty.\")\n",
        "    if len(eval_dataset) == 0:\n",
        "        logging.error(\"eval_dataset is empty.\")\n",
        "        raise ValueError(\"eval_dataset is empty.\")\n",
        "    if len(eval_dataset) < 50:\n",
        "        print(\"Warning: eval_dataset is very small (<50 rows).\")\n",
        "        logging.warning(\"eval_dataset is very small (<50 rows).\")\n",
        "\n",
        "    expected_variants = len(MODEL_NAMES) * len(HYPERPARAMS)\n",
        "    print(f\"Expecting to train {expected_variants} variants\")\n",
        "    logging.info(f\"Expecting to train {expected_variants} variants\")\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        # log tokenized dataset columns\n",
        "        train_cols = tokenized_datasets[model_name]['train'].column_names\n",
        "        eval_cols = tokenized_datasets[model_name]['eval'].column_names\n",
        "        print(f\"{model_name}: train cols={train_cols}, eval cols={eval_cols}\")\n",
        "        logging.info(f\"{model_name} columns: train={train_cols}, eval={eval_cols}\")\n",
        "\n",
        "        # log a sample label\n",
        "        sample = tokenized_datasets[model_name]['train'][0]\n",
        "        logging.info(f\"Sample train label for {model_name}: {sample['labels']}\")\n",
        "\n",
        "        for hyperparam_setting in HYPERPARAMS:\n",
        "            variant_name = f\"{model_name.replace('/', '_')}_{hyperparam_setting}\"\n",
        "            print(f\"\\n### Training {variant_name} ###\")\n",
        "            logging.info(f\"Starting training for {variant_name}\")\n",
        "\n",
        "            try:\n",
        "                da = DistractorAnalysis(model_name, hyperparam_setting)\n",
        "                da.train(tokenized_datasets)\n",
        "                logging.info(f\"trainer.train() completed for {variant_name}\")\n",
        "\n",
        "                # verify checkpoint\n",
        "                checkpoint_path = os.path.join(OUTPUT_DIR, f\"{variant_name}_best\")\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"✔️  Checkpoint found at {checkpoint_path}\")\n",
        "                    logging.info(f\"Checkpoint verified at {checkpoint_path}\")\n",
        "                else:\n",
        "                    print(f\"⚠️  Checkpoint not found at {checkpoint_path}\")\n",
        "                    logging.warning(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "\n",
        "                print(f\"Completed training for {variant_name}\")\n",
        "                logging.info(f\"Completed training for {variant_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Failed training for {variant_name}: {e}\"\n",
        "                print(error_msg)\n",
        "                logging.error(error_msg)\n",
        "                continue\n",
        "\n",
        "# Execute training\n",
        "train_all_models(tokenized_datasets, train_dataset, eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "26210d6821014ca3aaff2ab40e56f621",
            "5264489ac1d548f5ad097f3f3b78dfa0",
            "861d81e9b2b34b818dfe6e9b79c0bb54",
            "570f48474c8b4a7595c328eaabe3e69f",
            "298465ded6224bbd9e9f8df7dfe726a6",
            "19ded51998ee465dbfd0c3f084688ef4",
            "813d6dbe9be84620b4b4958f34a11c6c",
            "e5e274568b5c480f9533d911aaf9413f",
            "abdee0eba8414468a9db23545c2dff9a",
            "39c3371501f54128958b37013de73950",
            "8f6b2fdcaeaa45bc83a407fd64e159d8"
          ]
        },
        "id": "Zr0Auax_qr5X",
        "outputId": "c360bd55-5418-4eaa-f166-aeb8970411ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting train_all_models with 1 models × 3 hyperparam settings\n",
            "Raw train_dataset size: 13821 rows\n",
            "Raw eval_dataset size: 480 rows\n",
            "Expecting to train 3 variants\n",
            "sshleifer/distilbart-xsum-12-3: train cols=['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'], eval cols=['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n",
            "\n",
            "### Training sshleifer_distilbart-xsum-12-3_low ###\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26210d6821014ca3aaff2ab40e56f621",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/716M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnc-lonestar-tx\u001b[0m (\u001b[33mnc-lonestar-tx-university-of-north-texas\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_210703-mjzigies</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface/runs/mjzigies' target=\"_blank\">efficient-snow-17</a></strong> to <a href='https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface' target=\"_blank\">https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface/runs/mjzigies' target=\"_blank\">https://wandb.ai/nc-lonestar-tx-university-of-north-texas/huggingface/runs/mjzigies</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3456' max='3456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3456/3456 11:10, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.457400</td>\n",
              "      <td>1.420510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.432200</td>\n",
              "      <td>1.373131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.404600</td>\n",
              "      <td>1.385890</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔️  Checkpoint found at ./experiments/sshleifer_distilbart-xsum-12-3_low_best\n",
            "Completed training for sshleifer_distilbart-xsum-12-3_low\n",
            "\n",
            "### Training sshleifer_distilbart-xsum-12-3_medium ###\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5184' max='5184' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5184/5184 28:52, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.460400</td>\n",
              "      <td>1.411631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.414200</td>\n",
              "      <td>1.391720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.416200</td>\n",
              "      <td>1.395612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.401000</td>\n",
              "      <td>1.409791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>1.394400</td>\n",
              "      <td>1.406280</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔️  Checkpoint found at ./experiments/sshleifer_distilbart-xsum-12-3_medium_best\n",
            "Completed training for sshleifer_distilbart-xsum-12-3_medium\n",
            "\n",
            "### Training sshleifer_distilbart-xsum-12-3_high ###\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3102' max='4320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3102/4320 31:13 < 12:15, 1.65 it/s, Epoch 3.59/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.428400</td>\n",
              "      <td>1.397711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.409600</td>\n",
              "      <td>1.394878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.395700</td>\n",
              "      <td>1.396047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4320' max='4320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4320/4320 43:27, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.428400</td>\n",
              "      <td>1.397711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.409600</td>\n",
              "      <td>1.394878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.395700</td>\n",
              "      <td>1.396047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.392700</td>\n",
              "      <td>1.393892</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️  Checkpoint found at ./experiments/sshleifer_distilbart-xsum-12-3_high_best\n",
            "Completed training for sshleifer_distilbart-xsum-12-3_high\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 – Generate Predictions (log‐prob method)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def generate_predictions(eval_dataset):\n",
        "    \"\"\"\n",
        "    Run predict_logprob() for each model‐variant and write predictions.csv.\n",
        "    Assumes eval_dataset rows have:\n",
        "      'question', 'input_text', 'option_0'…'option_3', and 'correct_index'.\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    rows = []\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        for hp in HYPERPARAM_SETTINGS:\n",
        "            variant = f\"{model_name.replace('/', '_')}_{hp}\"\n",
        "            ckpt    = os.path.join(OUTPUT_DIR, f\"{variant}_best\")\n",
        "            print(f\"→ Predicting {variant}\")\n",
        "\n",
        "            # reload model\n",
        "            da = DistractorAnalysis(model_name, hp)\n",
        "            da.model.load_state_dict(\n",
        "                torch.load(os.path.join(ckpt,'pytorch_model.bin'), map_location=device)\n",
        "            )\n",
        "\n",
        "            # get preds + log‐probs\n",
        "            preds, logps = da.predict_logprob(eval_dataset, batch_size=1, device=device)\n",
        "\n",
        "            # build output rows\n",
        "            for i, ex in enumerate(eval_dataset):\n",
        "                rows.append({\n",
        "                    'question':             ex['question'],\n",
        "                    'model_name':           model_name,\n",
        "                    'variant':              hp,\n",
        "                    'predicted_choice':     int(preds[i]),\n",
        "                    'correct_choice_index': int(ex['correct_index']),\n",
        "                    'logprob_A':            float(logps[i,0]),\n",
        "                    'logprob_B':            float(logps[i,1]),\n",
        "                    'logprob_C':            float(logps[i,2]),\n",
        "                    'logprob_D':            float(logps[i,3]),\n",
        "                    'option_0':             ex['option_0'],\n",
        "                    'option_1':             ex['option_1'],\n",
        "                    'option_2':             ex['option_2'],\n",
        "                    'option_3':             ex['option_3'],\n",
        "                })\n",
        "            print(f\"✔ Done {variant}\")\n",
        "\n",
        "    # compile DataFrame & write out\n",
        "    predictions_df = pd.DataFrame(rows)\n",
        "    out_csv = os.path.join(OUTPUT_DIR, 'predictions.csv')\n",
        "    predictions_df.to_csv(out_csv, index=False)\n",
        "    print(f\"All predictions saved to {out_csv}\")\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "# Execute\n",
        "predictions_df = generate_predictions(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qia26wOjqu19",
        "outputId": "1ade2043-1033-4a78-9520-1ce1c749da45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Predicting sshleifer_distilbart-xsum-12-3_low\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Done sshleifer_distilbart-xsum-12-3_low\n",
            "→ Predicting sshleifer_distilbart-xsum-12-3_medium\n",
            "✔ Done sshleifer_distilbart-xsum-12-3_medium\n",
            "→ Predicting sshleifer_distilbart-xsum-12-3_high\n",
            "✔ Done sshleifer_distilbart-xsum-12-3_high\n",
            "All predictions saved to ./experiments/predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQAgBqwn5Et3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JUFo_lj-dfoH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FWrcMe4RTg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cvTbg8NRTeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ezU00G1HRTcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0xxjhVBRTaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMbXwQcNRTYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eu607HPYRYCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction Method 1 - Initial Probability Estimation"
      ],
      "metadata": {
        "id": "NLFf1i1bTjul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 - Distractor Analysis Class\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "\n",
        "class DistractorAnalysis:\n",
        "    def __init__(self, model_name, hyperparam_setting):\n",
        "        self.model_name = model_name\n",
        "        self.hyperparam_setting = hyperparam_setting\n",
        "\n",
        "        if hyperparam_setting not in HYPERPARAMS:\n",
        "            raise ValueError(f\"Invalid hyperparam_setting: {hyperparam_setting}\")\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model or tokenizer for {model_name}: {str(e)}\")\n",
        "\n",
        "        # Check choice tokens\n",
        "        choice_tokens = [\"A\", \"B\", \"C\", \"D\"]\n",
        "        self.choice_token_ids = []\n",
        "        for token in choice_tokens:\n",
        "            token_ids = self.tokenizer(token, add_special_tokens=False).input_ids\n",
        "            if len(token_ids) != 1:\n",
        "                raise ValueError(\n",
        "                    f\"Token '{token}' is not a single token in the tokenizer for {self.model_name}\"\n",
        "                )\n",
        "            self.choice_token_ids.append(token_ids[0])\n",
        "\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(\n",
        "                OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}\"\n",
        "            ),\n",
        "            per_device_train_batch_size=HYPERPARAMS[self.hyperparam_setting]['batch_size'],\n",
        "            per_device_eval_batch_size=HYPERPARAMS[self.hyperparam_setting]['batch_size'],\n",
        "            num_train_epochs=HYPERPARAMS[self.hyperparam_setting]['epochs'],\n",
        "            learning_rate=HYPERPARAMS[self.hyperparam_setting]['learning_rate'],\n",
        "            save_steps=1000,\n",
        "            save_total_limit=2,\n",
        "            logging_dir=os.path.join(\n",
        "                OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}_logs\"\n",
        "            ),\n",
        "            logging_steps=500,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=1000,\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"accuracy\",\n",
        "            greater_is_better=True,\n",
        "            remove_unused_columns=False\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Compute accuracy on first-token choice logits.\n",
        "        Handles both EvalPrediction and plain-tuple inputs.\n",
        "        \"\"\"\n",
        "        # Unpack predictions and labels\n",
        "        if hasattr(eval_pred, 'predictions') and hasattr(eval_pred, 'label_ids'):\n",
        "            raw_preds = eval_pred.predictions\n",
        "            labels = eval_pred.label_ids\n",
        "        else:\n",
        "            raw_preds, labels = eval_pred\n",
        "\n",
        "        # If raw_preds is a tuple/list (e.g., (logits, _)), take the first element\n",
        "        if isinstance(raw_preds, (tuple, list)):\n",
        "            raw_preds = raw_preds[0]\n",
        "\n",
        "        # If it’s a torch.Tensor, convert to numpy\n",
        "        if hasattr(raw_preds, 'numpy'):\n",
        "            raw_preds = raw_preds.numpy()\n",
        "\n",
        "        # If seq2seq LM output [batch, seq_len, vocab_size], get only first-token logits\n",
        "        if raw_preds.ndim == 3:\n",
        "            first_logits = raw_preds[:, 0, :]\n",
        "            # Keep only our A/B/C/D token columns\n",
        "            label_logits = first_logits[:, self.choice_token_ids]\n",
        "        else:\n",
        "            # Already [batch, num_labels]\n",
        "            label_logits = raw_preds\n",
        "\n",
        "        # Argmax and accuracy\n",
        "        preds = np.argmax(label_logits, axis=-1)\n",
        "        acc = np.mean(preds == labels)\n",
        "        return {\"accuracy\": acc}\n",
        "\n",
        "    def train(self, tokenized_datasets):\n",
        "        try:\n",
        "            logging.info(f\"Starting training for {self.model_name} with {self.hyperparam_setting}\")\n",
        "            trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=self.training_args,\n",
        "                train_dataset=tokenized_datasets[self.model_name]['train'],\n",
        "                eval_dataset=tokenized_datasets[self.model_name]['eval'],\n",
        "                compute_metrics=self.compute_metrics\n",
        "            )\n",
        "            trainer.train()\n",
        "            # Fix: use self.hyperparam_setting\n",
        "            trainer.save_model(os.path.join(\n",
        "                OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}_best\"\n",
        "            ))\n",
        "            self.tokenizer.save_pretrained(os.path.join(\n",
        "                OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}_best\"\n",
        "            ))\n",
        "            logging.info(f\"Training completed for {self.model_name} with {self.hyperparam_setting}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Training failed for {self.model_name} with {self.hyperparam_setting}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, dataset, batch_size=16, device='cuda'):\n",
        "        # Device fallback\n",
        "        actual_device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(actual_device).eval()\n",
        "\n",
        "        logging.info(f\"Running predict on {len(dataset)} samples with batch size {batch_size} on {actual_device}\")\n",
        "\n",
        "        all_logits = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(dataset), batch_size):\n",
        "                batch = dataset[i:i + batch_size]\n",
        "                # Create input tensors directly from batch data\n",
        "                input_ids = torch.tensor(batch['input_ids']).to(actual_device)\n",
        "                attention_mask = torch.tensor(batch['attention_mask']).to(actual_device)\n",
        "\n",
        "                # Set decoder_input_ids to start token\n",
        "                start_token_id = (\n",
        "                    self.tokenizer.pad_token_id\n",
        "                    if self.tokenizer.pad_token_id is not None\n",
        "                    else self.tokenizer.eos_token_id\n",
        "                )\n",
        "                decoder_input_ids = torch.full(\n",
        "                    (input_ids.size(0), 1),  # Fixed: use input_ids instead of undefined inputs\n",
        "                    start_token_id,\n",
        "                    device=actual_device\n",
        "                )\n",
        "\n",
        "                # Get model outputs\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,  # Fixed: use local variables\n",
        "                    attention_mask=attention_mask,  # Fixed: use local variables\n",
        "                    decoder_input_ids=decoder_input_ids\n",
        "                )\n",
        "\n",
        "                # Extract logits for the first generated token\n",
        "                logits = outputs.logits[:, 0, :]\n",
        "                choice_logits = logits[:, self.choice_token_ids]  # [batch_size, 4]\n",
        "                all_logits.append(choice_logits.cpu().numpy())\n",
        "\n",
        "        logits = np.vstack(all_logits)  # [num_samples, 4]\n",
        "        predictions = logits.argmax(axis=1)\n",
        "\n",
        "        return predictions, logits"
      ],
      "metadata": {
        "id": "ji8d5oGN459J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 - Prepare Datasets for Training\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "def prepare_datasets(human_ranked_df, mmlu_df):\n",
        "    # Verify required global variables\n",
        "    if 'MODEL_NAMES' not in globals():\n",
        "        raise NameError(\"MODEL_NAMES global variable not defined. Please run Cell #3 first.\")\n",
        "\n",
        "    option_columns = ['option_0', 'option_1', 'option_2', 'option_3']\n",
        "\n",
        "    # Map answer index to choice token (A, B, C, D)\n",
        "    def map_answer_to_choice(row, answer_col):\n",
        "        try:\n",
        "            answer_idx = int(row[answer_col])\n",
        "            if answer_idx not in [0, 1, 2, 3]:\n",
        "                return None\n",
        "            return chr(65 + answer_idx)  # 0 -> 'A', 1 -> 'B', 2 -> 'C', 3 -> 'D'\n",
        "        except (ValueError, TypeError, KeyError):\n",
        "            return None\n",
        "\n",
        "    # Validate columns in mmlu_df\n",
        "    missing_mmlu_cols = [col for col in ['question', 'correct_answer'] + option_columns if col not in mmlu_df.columns]\n",
        "    if missing_mmlu_cols:\n",
        "        print(f\"Warning: Missing columns in mmlu_df: {missing_mmlu_cols}. Available columns: {mmlu_df.columns.tolist()}\")\n",
        "        fallback_cols = ['choice1', 'choice2', 'choice3', 'choice4']\n",
        "        if all(col in mmlu_df.columns for col in fallback_cols):\n",
        "            print(\"Falling back to choice1, choice2, choice3, choice4 for mmlu_df\")\n",
        "            mmlu_df = mmlu_df.rename(columns={\n",
        "                'choice1': 'option_0',\n",
        "                'choice2': 'option_1',\n",
        "                'choice3': 'option_2',\n",
        "                'choice4': 'option_3'\n",
        "            })\n",
        "        else:\n",
        "            raise ValueError(\"Cannot proceed: mmlu_df missing required option columns\")\n",
        "\n",
        "    # Validate columns in human_ranked_df - using consistent approach with mmlu_df\n",
        "    missing_human_cols = [col for col in ['question', 'correct_answer'] + option_columns if col not in human_ranked_df.columns]\n",
        "    if missing_human_cols:\n",
        "        print(f\"Warning: Missing columns in human_ranked_df: {missing_human_cols}. Available columns: {human_ranked_df.columns.tolist()}\")\n",
        "        # Could add fallback logic here similar to mmlu_df if needed\n",
        "        raise ValueError(\"Cannot proceed: human_ranked_df missing required columns\")\n",
        "\n",
        "    # Process mmlu_df with safer type conversion\n",
        "    mmlu_df = mmlu_df.copy()\n",
        "    mmlu_df['correct_choice'] = mmlu_df.apply(lambda row: map_answer_to_choice(row, 'correct_answer'), axis=1)\n",
        "\n",
        "    # Safer conversion with validation\n",
        "    mmlu_df['correct_index'] = mmlu_df['correct_answer'].apply(\n",
        "        lambda x: int(x) if pd.notna(x) and str(x).strip().isdigit() else None\n",
        "    )\n",
        "\n",
        "    mmlu_df_dropped = mmlu_df.loc[(mmlu_df['correct_choice'].isna()) | (mmlu_df['correct_index'].isna())].copy()\n",
        "    mmlu_df = mmlu_df.loc[(mmlu_df['correct_choice'].notna()) & (mmlu_df['correct_index'].notna())].copy()\n",
        "    if len(mmlu_df_dropped) > 0:\n",
        "        print(f\"Dropped {len(mmlu_df_dropped)} rows from mmlu_df. Saved to ./dropped_mmlu_choice_rows.csv\")\n",
        "        mmlu_df_dropped.to_csv('./dropped_mmlu_choice_rows.csv', index=False)\n",
        "\n",
        "    # Process human_ranked_df with safer type conversion\n",
        "    human_ranked_df = human_ranked_df.copy()\n",
        "    human_ranked_df['correct_choice'] = human_ranked_df.apply(lambda row: map_answer_to_choice(row, 'correct_answer'), axis=1)\n",
        "\n",
        "    # Safer conversion with validation\n",
        "    human_ranked_df['correct_index'] = human_ranked_df['correct_answer'].apply(\n",
        "        lambda x: int(x) if pd.notna(x) and str(x).strip().isdigit() else None\n",
        "    )\n",
        "\n",
        "    human_ranked_df_dropped = human_ranked_df.loc[(human_ranked_df['correct_choice'].isna()) | (human_ranked_df['correct_index'].isna())].copy()\n",
        "    human_ranked_df = human_ranked_df.loc[(human_ranked_df['correct_choice'].notna()) & (human_ranked_df['correct_index'].notna())].copy()\n",
        "    if len(human_ranked_df_dropped) > 0:\n",
        "        print(f\"Dropped {len(human_ranked_df_dropped)} rows from human_ranked_df. Saved to ./dropped_human_ranked_choice_rows.csv\")\n",
        "        human_ranked_df_dropped.to_csv('./dropped_human_ranked_choice_rows.csv', index=False)\n",
        "\n",
        "    # Create input text\n",
        "    def create_input_text(row):\n",
        "        return f\"Question: {row['question']} Options: A: {row['option_0']} B: {row['option_1']} C: {row['option_2']} D: {row['option_3']}\"\n",
        "\n",
        "    mmlu_df['input_text'] = mmlu_df.apply(create_input_text, axis=1)\n",
        "    human_ranked_df['input_text'] = human_ranked_df.apply(create_input_text, axis=1)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_pandas(mmlu_df[['input_text', 'correct_choice', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'correct_index']])\n",
        "    eval_dataset = Dataset.from_pandas(human_ranked_df[['input_text', 'correct_choice', 'question', 'option_0', 'option_1', 'option_2', 'option_3', 'correct_index']])\n",
        "\n",
        "    # Remove pandas index column if present\n",
        "    for dataset in [train_dataset, eval_dataset]:\n",
        "        if '__index_level_0__' in dataset.column_names:\n",
        "            dataset = dataset.remove_columns('__index_level_0__')\n",
        "\n",
        "    # Tokenization\n",
        "    tokenized_datasets = {}\n",
        "    for model_name in MODEL_NAMES:\n",
        "        try:\n",
        "            print(f\"Tokenizing datasets for model: {model_name}\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "            def tokenize_function(examples):\n",
        "                model_inputs = tokenizer(\n",
        "                    examples['input_text'],\n",
        "                    max_length=512,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    return_tensors=None\n",
        "                )\n",
        "                # Tokenize correct_choice as labels\n",
        "                labels = tokenizer(examples['correct_choice'], add_special_tokens=False).input_ids\n",
        "                model_inputs['labels'] = [label[0] if len(label) == 1 else tokenizer.unk_token_id for label in labels]\n",
        "                # Set decoder_input_ids for seq2seq models\n",
        "                start_token = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "                model_inputs['decoder_input_ids'] = [[start_token]] * len(examples['input_text'])\n",
        "                return model_inputs\n",
        "\n",
        "            # Updated code to be more selective about which columns to keep\n",
        "            tokenized_train = train_dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                # Only keep columns needed by the model\n",
        "                remove_columns=[col for col in train_dataset.column_names\n",
        "                               if col not in ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']]\n",
        "            )\n",
        "            tokenized_eval = eval_dataset.map(\n",
        "                tokenize_function,\n",
        "                batched=True,\n",
        "                # Only keep columns needed by the model\n",
        "                remove_columns=[col for col in eval_dataset.column_names\n",
        "                               if col not in ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']]\n",
        "            )\n",
        "\n",
        "            # Verify the tokenized datasets have the required fields for DistractorAnalysis.predict\n",
        "            required_fields = ['input_ids', 'attention_mask', 'labels']\n",
        "            missing_train = [f for f in required_fields if f not in tokenized_train.column_names]\n",
        "            missing_eval = [f for f in required_fields if f not in tokenized_eval.column_names]\n",
        "\n",
        "            if missing_train or missing_eval:\n",
        "                print(f\"Warning: Missing fields for {model_name}. Train: {missing_train}, Eval: {missing_eval}\")\n",
        "                if missing_train or missing_eval:\n",
        "                    raise ValueError(f\"Tokenized datasets missing required fields for model {model_name}\")\n",
        "\n",
        "            tokenized_datasets[model_name] = {'train': tokenized_train, 'eval': tokenized_eval}\n",
        "            print(f\"Tokenized datasets for {model_name}: train={len(tokenized_train)} rows, eval={len(tokenized_eval)} rows\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to tokenize for {model_name}: {str(e)}\")\n",
        "            logging.error(f\"Failed to tokenize for {model_name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    # Log sizes for sanity check\n",
        "    print(f\"Original sizes - MMLU: {len(mmlu_df)}, Human Ranked: {len(human_ranked_df)}\")\n",
        "    print(f\"After processing - Train dataset: {len(train_dataset)}, Eval dataset: {len(eval_dataset)}\")\n",
        "\n",
        "    return tokenized_datasets, train_dataset, eval_dataset\n",
        "\n",
        "# Execute\n",
        "try:\n",
        "    tokenized_datasets, train_dataset, eval_dataset = prepare_datasets(human_ranked_df, mmlu_df)\n",
        "    print(\"Datasets prepared successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preparing datasets: {str(e)}\")\n",
        "    logging.error(f\"Error preparing datasets: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "PfmfSQcr48yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 - Train Models\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def train_all_models(tokenized_datasets, train_dataset, eval_dataset):\n",
        "    # set up logging to file\n",
        "    log_file = os.path.join(OUTPUT_DIR, 'training_log.txt')\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # start message\n",
        "    print(f\"Starting train_all_models with {len(MODEL_NAMES)} models × {len(HYPERPARAMS)} hyperparam settings\")\n",
        "    logging.info(f\"MODEL_NAMES={MODEL_NAMES}, HYPERPARAMS={list(HYPERPARAMS.keys())}\")\n",
        "\n",
        "    # dataset size checks\n",
        "    print(f\"Raw train_dataset size: {len(train_dataset)} rows\")\n",
        "    print(f\"Raw eval_dataset size: {len(eval_dataset)} rows\")\n",
        "    logging.info(f\"Raw train_dataset size: {len(train_dataset)} rows\")\n",
        "    logging.info(f\"Raw eval_dataset size: {len(eval_dataset)} rows\")\n",
        "\n",
        "    if len(train_dataset) == 0:\n",
        "        logging.error(\"train_dataset is empty.\")\n",
        "        raise ValueError(\"train_dataset is empty.\")\n",
        "    if len(eval_dataset) == 0:\n",
        "        logging.error(\"eval_dataset is empty.\")\n",
        "        raise ValueError(\"eval_dataset is empty.\")\n",
        "    if len(eval_dataset) < 50:\n",
        "        print(\"Warning: eval_dataset is very small (<50 rows).\")\n",
        "        logging.warning(\"eval_dataset is very small (<50 rows).\")\n",
        "\n",
        "    expected_variants = len(MODEL_NAMES) * len(HYPERPARAMS)\n",
        "    print(f\"Expecting to train {expected_variants} variants\")\n",
        "    logging.info(f\"Expecting to train {expected_variants} variants\")\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        # log tokenized dataset columns\n",
        "        train_cols = tokenized_datasets[model_name]['train'].column_names\n",
        "        eval_cols = tokenized_datasets[model_name]['eval'].column_names\n",
        "        print(f\"{model_name}: train cols={train_cols}, eval cols={eval_cols}\")\n",
        "        logging.info(f\"{model_name} columns: train={train_cols}, eval={eval_cols}\")\n",
        "\n",
        "        # log a sample label\n",
        "        sample = tokenized_datasets[model_name]['train'][0]\n",
        "        logging.info(f\"Sample train label for {model_name}: {sample['labels']}\")\n",
        "\n",
        "        for hyperparam_setting in HYPERPARAMS:\n",
        "            variant_name = f\"{model_name.replace('/', '_')}_{hyperparam_setting}\"\n",
        "            print(f\"\\n### Training {variant_name} ###\")\n",
        "            logging.info(f\"Starting training for {variant_name}\")\n",
        "\n",
        "            try:\n",
        "                da = DistractorAnalysis(model_name, hyperparam_setting)\n",
        "                da.train(tokenized_datasets)\n",
        "                logging.info(f\"trainer.train() completed for {variant_name}\")\n",
        "\n",
        "                # verify checkpoint\n",
        "                checkpoint_path = os.path.join(OUTPUT_DIR, f\"{variant_name}_best\")\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    print(f\"✔️  Checkpoint found at {checkpoint_path}\")\n",
        "                    logging.info(f\"Checkpoint verified at {checkpoint_path}\")\n",
        "                else:\n",
        "                    print(f\"⚠️  Checkpoint not found at {checkpoint_path}\")\n",
        "                    logging.warning(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "\n",
        "                print(f\"Completed training for {variant_name}\")\n",
        "                logging.info(f\"Completed training for {variant_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Failed training for {variant_name}: {e}\"\n",
        "                print(error_msg)\n",
        "                logging.error(error_msg)\n",
        "                continue\n",
        "\n",
        "# Execute training\n",
        "train_all_models(tokenized_datasets, train_dataset, eval_dataset)"
      ],
      "metadata": {
        "id": "y_ekf0X54_Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 - Generate predictions\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import logging\n",
        "import os\n",
        "\n",
        "def generate_predictions(eval_dataset, tokenized_datasets):\n",
        "    log_file = os.path.join(OUTPUT_DIR, 'predictions_log.txt')\n",
        "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results = []\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        for hyperparam_setting in HYPERPARAMS:\n",
        "            variant_name = f\"{model_name.replace('/', '_')}_{hyperparam_setting}\"\n",
        "            checkpoint_path = os.path.join(OUTPUT_DIR, f\"{variant_name}_best\")\n",
        "\n",
        "            print(f\"Generating predictions for {variant_name}...\")\n",
        "            logging.info(f\"Generating predictions for {variant_name}\")\n",
        "\n",
        "            try:\n",
        "                da = DistractorAnalysis(model_name, hyperparam_setting)\n",
        "                checkpoint_file = os.path.join(checkpoint_path, 'pytorch_model.bin')\n",
        "                if not os.path.exists(checkpoint_file):\n",
        "                    raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_file}\")\n",
        "\n",
        "                da.model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
        "\n",
        "                # Use the tokenized eval dataset specific to this model\n",
        "                # This ensures we're using data with the right format for the model\n",
        "                tokenized_eval = tokenized_datasets[model_name]['eval']\n",
        "\n",
        "                predictions, choice_logits = da.predict(\n",
        "                    tokenized_eval,\n",
        "                    batch_size=HYPERPARAMS[hyperparam_setting]['batch_size'],\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                # To match tokenized predictions with original data attributes\n",
        "                for i in range(len(predictions)):\n",
        "                    result = {\n",
        "                        'question': eval_dataset[i]['question'],\n",
        "                        'model_name': model_name,\n",
        "                        'variant': hyperparam_setting,\n",
        "                        'predicted_choice': int(predictions[i]),\n",
        "                        'correct_choice_index': int(eval_dataset[i]['correct_index']),\n",
        "                        'logit_A': float(choice_logits[i][0]),\n",
        "                        'logit_B': float(choice_logits[i][1]),\n",
        "                        'logit_C': float(choice_logits[i][2]),\n",
        "                        'logit_D': float(choice_logits[i][3]),\n",
        "                        'option_0': eval_dataset[i]['option_0'],\n",
        "                        'option_1': eval_dataset[i]['option_1'],\n",
        "                        'option_2': eval_dataset[i]['option_2'],\n",
        "                        'option_3': eval_dataset[i]['option_3']\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                print(f\"Completed predictions for {variant_name}\")\n",
        "                logging.info(f\"Completed predictions for {variant_name}\")\n",
        "            except Exception as e:\n",
        "                error_msg = f\"Failed predictions for {variant_name}: {str(e)}\"\n",
        "                print(error_msg)\n",
        "                logging.error(error_msg)\n",
        "                continue\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    output_csv = os.path.join(OUTPUT_DIR, 'predictions.csv')\n",
        "    results_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "    logging.info(f\"Predictions saved to {output_csv}\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Execute the function with both the original evaluation dataset (for metadata)\n",
        "# and the tokenized datasets (for model input)\n",
        "predictions_df = generate_predictions(eval_dataset, tokenized_datasets)"
      ],
      "metadata": {
        "id": "fk2LpTYi5B1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prediction Method 2 - Classification Head (Encoder Only)"
      ],
      "metadata": {
        "id": "xw8xAZnWTt-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 – Distractor Analysis Class (Classification Head)\n",
        "\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# 4-way classification head on top of T5/BART encoders\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        if 't5' in model_name.lower():\n",
        "            from transformers import T5Model\n",
        "            self.encoder = T5Model.from_pretrained(model_name).get_encoder()\n",
        "        else:\n",
        "            from transformers import BartModel\n",
        "            self.encoder = BartModel.from_pretrained(model_name).get_encoder()\n",
        "        hidden_size = self.encoder.config.d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Encode and pool <s> token\n",
        "        enc = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = self.dropout(enc.last_hidden_state[:, 0, :])\n",
        "        logits = self.classifier(pooled)          # [batch, num_labels]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        return {'logits': logits, 'loss': loss}\n",
        "\n",
        "class DistractorAnalysis:\n",
        "    def __init__(self, model_name, hyperparam_setting):\n",
        "        self.model_name = model_name\n",
        "        self.hyperparam_setting = hyperparam_setting\n",
        "\n",
        "        if hyperparam_setting not in HYPERPARAMS:\n",
        "            raise ValueError(f\"Invalid hyperparam_setting: {hyperparam_setting}\")\n",
        "\n",
        "        # Load tokenizer + classification model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = ClassificationModel(model_name, NUM_LABELS)\n",
        "\n",
        "        # TrainingArguments setup\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{hyperparam_setting}\"),\n",
        "            per_device_train_batch_size=HYPERPARAMS[hyperparam_setting]['batch_size'],\n",
        "            per_device_eval_batch_size= HYPERPARAMS[hyperparam_setting]['batch_size'],\n",
        "            num_train_epochs=         HYPERPARAMS[hyperparam_setting]['epochs'],\n",
        "            learning_rate=            HYPERPARAMS[hyperparam_setting]['learning_rate'],\n",
        "            evaluation_strategy=      \"steps\",\n",
        "            eval_steps=               1000,\n",
        "            logging_steps=            500,\n",
        "            save_steps=               1000,\n",
        "            save_total_limit=         2,\n",
        "            load_best_model_at_end=   True,\n",
        "            metric_for_best_model=    \"accuracy\",\n",
        "            greater_is_better=        True,\n",
        "            remove_unused_columns=    False,\n",
        "            logging_dir=os.path.join(OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{hyperparam_setting}_logs\"),\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        # Unpack EvalPrediction or tuple\n",
        "        if hasattr(eval_pred, 'predictions') and hasattr(eval_pred, 'label_ids'):\n",
        "            logits = eval_pred.predictions\n",
        "            labels = eval_pred.label_ids\n",
        "        else:\n",
        "            logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = np.mean(preds == labels)\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    def train(self, tokenized_datasets):\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=tokenized_datasets[self.model_name]['train'],\n",
        "            eval_dataset= tokenized_datasets[self.model_name]['eval'],\n",
        "            compute_metrics=self.compute_metrics\n",
        "        )\n",
        "        trainer.train()\n",
        "        ckpt = os.path.join(OUTPUT_DIR, f\"{self.model_name.replace('/', '_')}_{self.hyperparam_setting}_best\")\n",
        "        trainer.save_model(ckpt)\n",
        "        self.tokenizer.save_pretrained(ckpt)\n",
        "\n",
        "    def predict(self, dataset, batch_size=16, device='cuda'):\n",
        "        # Classification: no decoder inputs needed\n",
        "        dev = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(dev).eval()\n",
        "\n",
        "        all_logits, all_preds = [], []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(dataset), batch_size):\n",
        "                batch = dataset[i : i + batch_size]\n",
        "                inputs = self.tokenizer(\n",
        "                    batch['input_text'],\n",
        "                    padding='max_length',\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    return_tensors='pt'\n",
        "                ).to(dev)\n",
        "\n",
        "                out = self.model(input_ids=inputs.input_ids,\n",
        "                                 attention_mask=inputs.attention_mask)\n",
        "                logits = out['logits']                    # [batch, NUM_LABELS]\n",
        "                preds  = torch.argmax(logits, dim=-1)     # [batch]\n",
        "\n",
        "                all_logits.append(logits.cpu().numpy())\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "        logits_arr = np.vstack(all_logits)\n",
        "        preds_arr  = np.concatenate(all_preds)\n",
        "        return preds_arr, logits_arr"
      ],
      "metadata": {
        "id": "0OsQ2_8BTyxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7 – Prepare Datasets\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def prepare_datasets(human_ranked_df, mmlu_df):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      tokenized_datasets: {model_name: {'train','eval'}}\n",
        "      train_dataset (HF Dataset), eval_dataset (HF Dataset)\n",
        "    \"\"\"\n",
        "    option_cols = ['option_0','option_1','option_2','option_3']\n",
        "\n",
        "    # Build unified input_text and integer labels\n",
        "    def make_input(qrow):\n",
        "        opts = \" \".join(f\"{chr(65+i)}: {qrow[c]}\" for i,c in enumerate(option_cols))\n",
        "        return f\"Question: {qrow['question']} Options: {opts}\"\n",
        "\n",
        "    mmlu_df['input_text'] = mmlu_df.apply(make_input, axis=1)\n",
        "    mmlu_df['labels']     = mmlu_df['correct_answer'].astype(int)\n",
        "\n",
        "    human_ranked_df['input_text'] = human_ranked_df.apply(make_input, axis=1)\n",
        "    human_ranked_df['labels']     = human_ranked_df['correct_answer'].astype(int)\n",
        "\n",
        "    # Create HF Datasets\n",
        "    train_dataset = Dataset.from_pandas(mmlu_df[['input_text','labels']].reset_index(drop=True))\n",
        "    eval_dataset  = Dataset.from_pandas(human_ranked_df[['input_text','labels','question',\n",
        "                                                          'option_0','option_1','option_2','option_3']].reset_index(drop=True))\n",
        "\n",
        "    # Tokenize per model\n",
        "    tokenized_datasets = {}\n",
        "    for model_name in MODEL_NAMES:\n",
        "        tok = AutoTokenizer.from_pretrained(model_name)\n",
        "        def tokenize_fn(examples):\n",
        "            out = tok(examples['input_text'],\n",
        "                      padding='max_length',\n",
        "                      truncation=True,\n",
        "                      max_length=512)\n",
        "            out['labels'] = examples['labels']\n",
        "            return out\n",
        "\n",
        "        ttrain = train_dataset.map(tokenize_fn, batched=True, remove_columns=train_dataset.column_names)\n",
        "        teval  = eval_dataset.map( tokenize_fn, batched=True, remove_columns= eval_dataset.column_names)\n",
        "        tokenized_datasets[model_name] = {'train': ttrain, 'eval': teval}\n",
        "        print(f\"Tokenized for {model_name}: train={len(ttrain)}, eval={len(teval)}\")\n",
        "\n",
        "    return tokenized_datasets, train_dataset, eval_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Execute\n",
        "try:\n",
        "    tokenized_datasets, train_dataset, eval_dataset = prepare_datasets(human_ranked_df, mmlu_df)\n",
        "    print(\"Datasets prepared successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preparing datasets: {str(e)}\")\n",
        "    logging.error(f\"Error preparing datasets: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "tSPsBpd4T2tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8 – Train All Models\n",
        "\n",
        "import os\n",
        "import logging\n",
        "\n",
        "def train_all_models(tokenized_datasets, train_dataset, eval_dataset):\n",
        "    # Setup logging once\n",
        "    log_file = os.path.join(OUTPUT_DIR, 'training_log.txt')\n",
        "    logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Sanity checks\n",
        "    if len(train_dataset)==0 or len(eval_dataset)==0:\n",
        "        raise ValueError(\"Empty train or eval dataset.\")\n",
        "\n",
        "    # Loop variants\n",
        "    for model_name in MODEL_NAMES:\n",
        "        for hp in HYPERPARAM_SETTINGS:\n",
        "            variant_name = f\"{model_name.replace('/', '_')}_{hp}\"\n",
        "            print(f\"Training {variant_name}...\")\n",
        "            logging.info(f\"Training {variant_name}\")\n",
        "            try:\n",
        "                da = DistractorAnalysis(model_name, hp)\n",
        "                da.train(tokenized_datasets)\n",
        "                print(f\"✓ Completed {variant_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Failed {variant_name}: {e}\")\n",
        "                logging.error(f\"{variant_name} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "# Execute\n",
        "train_all_models(tokenized_datasets, train_dataset, eval_dataset)"
      ],
      "metadata": {
        "id": "XVTK-1J3T4PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 – Generate Predictions\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def generate_predictions(eval_dataset):\n",
        "    \"\"\"\n",
        "    Runs each model‐variant over eval_dataset, collects preds+logits,\n",
        "    and writes a single predictions.csv with one row per question/variant.\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    rows = []\n",
        "\n",
        "    for model_name in MODEL_NAMES:\n",
        "        for hp in HYPERPARAM_SETTINGS:\n",
        "            variant_name = f\"{model_name.replace('/', '_')}_{hp}\"\n",
        "            ckpt_dir = os.path.join(OUTPUT_DIR, f\"{variant_name}_best\")\n",
        "            model_bin = os.path.join(ckpt_dir, 'pytorch_model.bin')\n",
        "            print(f\"Predicting {variant_name}…\")\n",
        "\n",
        "            da = DistractorAnalysis(model_name, hp)\n",
        "            da.model.load_state_dict(torch.load(model_bin, map_location=device))\n",
        "\n",
        "            preds, logits = da.predict(eval_dataset, batch_size=HYPERPARAMS[hp]['batch_size'], device=device)\n",
        "\n",
        "            # Build rows\n",
        "            for i, qrow in enumerate(eval_dataset):\n",
        "                rows.append({\n",
        "                    'question':            qrow['question'],\n",
        "                    'model_name':          model_name,\n",
        "                    'variant':             hp,\n",
        "                    'predicted_choice':    int(preds[i]),\n",
        "                    'correct_choice_index': int(qrow['labels']),\n",
        "                    'logit_0':             float(logits[i,0]),\n",
        "                    'logit_1':             float(logits[i,1]),\n",
        "                    'logit_2':             float(logits[i,2]),\n",
        "                    'logit_3':             float(logits[i,3]),\n",
        "                    'option_0':            qrow['option_0'],\n",
        "                    'option_1':            qrow['option_1'],\n",
        "                    'option_2':            qrow['option_2'],\n",
        "                    'option_3':            qrow['option_3'],\n",
        "                })\n",
        "            print(f\"✓ Done {variant_name}\")\n",
        "\n",
        "    # Compile DataFrame + write CSV\n",
        "    predictions_df = pd.DataFrame(rows)\n",
        "    out_csv = os.path.join(OUTPUT_DIR, 'predictions.csv')\n",
        "    predictions_df.to_csv(out_csv, index=False)\n",
        "    print(f\"All predictions saved to {out_csv}\")\n",
        "    return predictions_df\n",
        "\n",
        "# Execute\n",
        "predictions_df = generate_predictions(eval_dataset)"
      ],
      "metadata": {
        "id": "AdNlIXFlT3_n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}